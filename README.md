# Encoder
# Custom Encoder for Masked Language Modeling

This repository contains an implementation of a custom encoder architecture tailored for masked language modeling tasks. The implementation includes custom self-attention mechanisms, encoder blocks, and a preprocessing pipeline.

## Features

- **Custom Self-Attention Mechanism:** Implemented a self-attention module from scratch to capture contextual dependencies within input sequences.
  
- **Encoder Architecture:** Developed a flexible encoder architecture comprising multiple layers of base encoders, incorporating self-attention, feed-forward layers, and layer normalization.
  
- **Preprocessing Pipeline:** Designed a preprocessing pipeline for masked language modeling tasks, which involves word piece tokenization, selective masking of tokens, and embedding of words and positions.

